{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-workstation",
   "metadata": {},
   "source": [
    "# Assignment-2: Term Level-Inverted Index\n",
    "\n",
    "# Spark - Standalone deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-surgery",
   "metadata": {},
   "source": [
    "Spark can be executed in a standalone mode, similarly to MapReduce. However, some slight changes in the configuration are required. More specifically, we will have to simply uncomment the lines that define the <code>PYSPARK_DRIVER_PYTHON</code> and <code>PYSPARK_DRIVER_PYTHON_OPTS</code> parameters:\n",
    "\n",
    "<ol>\n",
    "    <li style=\"padding:10px 0 5px 0\">Open the <code>bashrc</code> file for editting: <code>sudo gedit ~/.bashrc</code>\n",
    "    After line 134 apply the following changes:<br><br>\n",
    "        <code>#Hadoop Related Options\n",
    "export HADOOP_HOME=/home/hdoop/hadoop-3.2.1\n",
    "export HADOOP_INSTALL=$HADOOP_HOME\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
    "export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\n",
    "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n",
    "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</code>\n",
    "\n",
    "<code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "export SBT_HOME=/usr/share/sbt-launcher-packaging/bin/sbt-launch.jar  \n",
    "export SPARK_HOME=/usr/lib/spark\n",
    "export PATH=$PATH:$JAVA_HOME/bin\n",
    "export PATH=$PATH:$SBT_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "#export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "#export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "export PYSPARK_PYTHON=/usr/bin/python3.8\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH</code>\n",
    "    </li>\n",
    "    <li style=\"padding:10px 0 5px 0\">Apply the changes to <code>bashrc</code> immediately: <code>source ~/.bashrc</code>.</li>\n",
    "    <li style=\"padding:10px 0 5px 0\">We <b>do not have</b> write the Python code into a <code>.py</code> file. </li>\n",
    "    <li style=\"padding:10px 0 5px 0\">The python code can be executed by PySpark directly from this notebook.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-somewhere",
   "metadata": {},
   "source": [
    "# <code>SparkSession</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addressed-charity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.1.1\n",
      "PySpark Version: 3.1.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local[4]') \\\n",
    "        .appName(\"IHU_Spark_Standalone_TestApp\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(\"Spark Version: \" + sc.version)\n",
    "print(\"PySpark Version: \" + pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-tanzania",
   "metadata": {},
   "source": [
    "# Test Data-posts.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-campaign",
   "metadata": {},
   "source": [
    "<b>Due to large file size of posts.csv, unnecessary columns have been removed manually using Libre Office.</b>\n",
    "\n",
    "The Unofficial Apple Weblog (TUAW)- The inverted index will be constructed by taking into consideration only the titles of the blog posts.\n",
    "\n",
    "Copy input data files to HDFS. The <code>-f</code> switch of <code>copyFromLocal</code> overwrites the destination file (in case it exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "solved-fluid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-11 13:26:20,806 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyFromLocal -f /home/bdccuser/notebooks/spark/data/posts.csv /user/bdccuser/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-parker",
   "metadata": {},
   "source": [
    "# Working with DataFrames\n",
    "\n",
    "## 1. Creating Dataframes\n",
    "\n",
    "### 1.1. Creating Dataframes from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "skilled-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from an external CSV file (in HDFS)\n",
    "df_hdfs = sc.read.option('delimiter', ',').option('header', 'true')\\\n",
    "    .csv(\"/user/bdccuser/posts.csv\")\n",
    "\n",
    "# Create a DataFrame from an external text file (in Local FS)\n",
    "df_local = sc.read.option('delimiter', ',').option('header', 'true')\\\n",
    "    .csv(\"file:////home/bdccuser/notebooks/spark/data/posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "typical-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the dataframe (columns and column names - headers)\n",
    "df_hdfs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verified-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               Title|\n",
      "+--------------------+\n",
      "|First Look: Guita...|\n",
      "|TUAW Tip: use the...|\n",
      "|Boxee is updated ...|\n",
      "|Found Footage: A ...|\n",
      "|Concept: the iPod...|\n",
      "|Talkcast live ton...|\n",
      "|Black Friday: Bes...|\n",
      "|uTorrent for Mac ...|\n",
      "|$1.7 million for ...|\n",
      "|Details emerge on...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a small number of records of the Dataframe\n",
    "df_hdfs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greater-sustainability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17831\n"
     ]
    }
   ],
   "source": [
    "print(df_hdfs.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-concept",
   "metadata": {},
   "source": [
    "### 1.2. Creating RDDs from DataFrames\n",
    "\n",
    "DataFrame does not support <code>map</code> transformations. Therefore, to apply <code>map</code> the DataFrame must be firstly converted to an RDD.\n",
    "\n",
    "The opposite job can be performed by converting a DataFrame to an RDD through the <code>fromDF()</code> method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "material-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Title='First Look: Guitar Rock Tour for iPhone'), Row(Title='TUAW Tip: use the Help menu to search Safari bookmarks and history'), Row(Title='Boxee is updated for Apple TV 2.3'), Row(Title='Found Footage: A working NeXT Cube'), Row(Title='Concept: the iPod shuffle bracelet'), Row(Title='Talkcast live tonight at 10pm ET'), Row(Title='Black Friday: Best Buy discounting up to $150 off, Apple retail will match prices'), Row(Title='uTorrent for Mac beta officially released'), Row(Title='$1.7 million for Greensboro Apple Store permit'), Row(Title='Details emerge on future Apple Stores'), Row(Title='First picture of the Greensboro, NC Apple Store under construction'), Row(Title='Ask TUAW: Reinstalling, auto-saving, license keeping and more'), Row(Title='Sneak Preview - Freeverse Flick Fishing 1.2'), Row(Title='Freeverse goes Flick Fishing'), Row(Title='Last chance for two App Store freebies'), Row(Title='iPhone hackers achieve a milestone: Linux boot'), Row(Title='Discounts, doorbusters, and more: TUAW sampler of holiday savings'), Row(Title='Love that yellow sticky note...'), Row(Title='Apple Store Australia posts Apple holiday sale discounts'), Row(Title='TUAW Tip of the Day')]\n"
     ]
    }
   ],
   "source": [
    "df_hdfs = sc.read.option('delimiter', ',').option('header', 'true')\\\n",
    "    .csv(\"/user/bdccuser/posts*\")\n",
    "\n",
    "rdd = df_hdfs.rdd\n",
    "\n",
    "print(rdd.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "promising-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'First Look: Guitar Rock Tour for iPhone', 'TUAW Tip: use the Help menu to search Safari bookmarks and history', 'Boxee is updated for Apple TV 2.3', 'Found Footage: A working NeXT Cube', 'Concept: the iPod shuffle bracelet', 'Talkcast live tonight at 10pm ET', '\"Black Friday: Best Buy discounting up to $150 off, Apple retail will match prices\"', 'uTorrent for Mac beta officially released', '$1.7 million for Greensboro Apple Store permit', 'Details emerge on future Apple Stores', '\"First picture of the Greensboro, NC Apple Store under construction\"', '\"Ask TUAW: Reinstalling, auto-saving, license keeping and more\"', 'Sneak Preview - Freeverse Flick Fishing 1.2', 'Freeverse goes Flick Fishing', 'Last chance for two App Store freebies', 'iPhone hackers achieve a milestone: Linux boot', '\"Discounts, doorbusters, and more: TUAW sampler of holiday savings\"', 'Love that yellow sticky note...', 'Apple Store Australia posts Apple holiday sale discounts']\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from an external text file (in HDFS)\n",
    "rdd_posts_hdfs = sc.sparkContext.textFile(\"/user/bdccuser/posts*\")\n",
    "\n",
    "print(rdd_posts_hdfs.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-poultry",
   "metadata": {},
   "source": [
    "## 2. Repartitioning Dataframes\n",
    "\n",
    "An Dataframe may be repartitioned across the nodes of the cluster by using <code>repartition(T)</code>. This command shuffles data from all nodes (also called full shuffle) and splits it into <code>T</code> segments.\n",
    "<br><br>\n",
    "<b>Note:</b> <code>repartition()</code> is a very expensive operation as it shuffles data from all nodes in a cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "selective-lightweight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions before repartitioning:1\n",
      "Partitions after repartitioning:8\n"
     ]
    }
   ],
   "source": [
    "print(\"Partitions before repartitioning:\" + str(df_hdfs.rdd.getNumPartitions()))\n",
    "\n",
    "df_hdfs_2 = df_hdfs.repartition(8)\n",
    "\n",
    "print(\"Partitions after repartitioning:\" + str(df_hdfs_2.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-steering",
   "metadata": {},
   "source": [
    "## 3. Transformations\n",
    "\n",
    "A Dataframe can be also accompanied by a custom schema. A schema is a structure that describes the columns of the Dataframe including the column names, data types, metadata etc. It provides much greater control over the columns of a Dataframe and makes the comply with db-like rules.\n",
    "\n",
    "Therefore, the application of a schema guarantees the correct execution of queries, filters, sort operations, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crude-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               Title|\n",
      "+--------------------+\n",
      "|First Look: Guita...|\n",
      "|TUAW Tip: use the...|\n",
      "|Boxee is updated ...|\n",
      "|Found Footage: A ...|\n",
      "|Concept: the iPod...|\n",
      "|Talkcast live ton...|\n",
      "|Black Friday: Bes...|\n",
      "|uTorrent for Mac ...|\n",
      "|$1.7 million for ...|\n",
      "|Details emerge on...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, DoubleType\n",
    "\n",
    "custom_schema = StructType([\n",
    "#     StructField(\"DocId\", IntegerType(), True, metadata={\"desc\": \"document id\"}),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = sc.read.option('delimiter', ',').option('header', 'true') \\\n",
    "    .schema(custom_schema) \\\n",
    "    .csv(\"/user/bdccuser/posts*\")\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-cologne",
   "metadata": {},
   "source": [
    "### MAP Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tested-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title',\n",
       " 'First Look: Guitar Rock Tour for iPhone',\n",
       " 'TUAW Tip: use the Help menu to search Safari bookmarks and history',\n",
       " 'Boxee is updated for Apple TV 2.3',\n",
       " 'Found Footage: A working NeXT Cube',\n",
       " 'Concept: the iPod shuffle bracelet',\n",
       " 'Talkcast live tonight at 10pm ET',\n",
       " '\"Black Friday: Best Buy discounting up to $150 off, Apple retail will match prices\"',\n",
       " 'uTorrent for Mac beta officially released',\n",
       " '$1.7 million for Greensboro Apple Store permit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_posts_hdfs.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sexual-server",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Posts:  [['Title'], ['First', 'Look:', 'Guitar', 'Rock', 'Tour', 'for', 'iPhone'], ['TUAW', 'Tip:', 'use', 'the', 'Help', 'menu', 'to', 'search', 'Safari', 'bookmarks', 'and', 'history'], ['Boxee', 'is', 'updated', 'for', 'Apple', 'TV', '2.3'], ['Found', 'Footage:', 'A', 'working', 'NeXT', 'Cube'], ['Concept:', 'the', 'iPod', 'shuffle', 'bracelet'], ['Talkcast', 'live', 'tonight', 'at', '10pm', 'ET'], ['\"Black', 'Friday:', 'Best', 'Buy', 'discounting', 'up', 'to', '$150', 'off,', 'Apple', 'retail', 'will', 'match', 'prices\"'], ['uTorrent', 'for', 'Mac', 'beta', 'officially', 'released'], ['$1.7', 'million', 'for', 'Greensboro', 'Apple', 'Store', 'permit']]\n"
     ]
    }
   ],
   "source": [
    "# An example with lambda function\n",
    "rdd_posts = rdd_posts_hdfs.map(lambda x: x.split(\" \"))\n",
    "print(\"RDD Posts: \", rdd_posts.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "engaging-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Posts:  [['TITLE'], ['FIRST', 'LOOK:', 'GUITAR', 'ROCK', 'TOUR', 'FOR', 'IPHONE'], ['TUAW', 'TIP:', 'USE', 'THE', 'HELP', 'MENU', 'TO', 'SEARCH', 'SAFARI', 'BOOKMARKS', 'AND', 'HISTORY'], ['BOXEE', 'IS', 'UPDATED', 'FOR', 'APPLE', 'TV', '2.3'], ['FOUND', 'FOOTAGE:', 'A', 'WORKING', 'NEXT', 'CUBE'], ['CONCEPT:', 'THE', 'IPOD', 'SHUFFLE', 'BRACELET'], ['TALKCAST', 'LIVE', 'TONIGHT', 'AT', '10PM', 'ET'], ['\"BLACK', 'FRIDAY:', 'BEST', 'BUY', 'DISCOUNTING', 'UP', 'TO', '$150', 'OFF,', 'APPLE', 'RETAIL', 'WILL', 'MATCH', 'PRICES\"'], ['UTORRENT', 'FOR', 'MAC', 'BETA', 'OFFICIALLY', 'RELEASED'], ['$1.7', 'MILLION', 'FOR', 'GREENSBORO', 'APPLE', 'STORE', 'PERMIT']]\n"
     ]
    }
   ],
   "source": [
    "# An example with custom function\n",
    "def upper_list (x) :\n",
    "    for k in range(len(x)):\n",
    "        x[k] = x[k].upper()\n",
    "    return x\n",
    "\n",
    "def equiv_fun (x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "rdd_posts_2 = rdd_posts.map(upper_list)\n",
    "print(\"RDD Posts: \", rdd_posts_2.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ahead-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Title='First Look: Guitar Rock Tour for iPhone'), Row(Title='TUAW Tip: use the Help menu to search Safari bookmarks and history'), Row(Title='Boxee is updated for Apple TV 2.3'), Row(Title='Found Footage: A working NeXT Cube'), Row(Title='Concept: the iPod shuffle bracelet'), Row(Title='Talkcast live tonight at 10pm ET'), Row(Title='Black Friday: Best Buy discounting up to $150 off, Apple retail will match prices'), Row(Title='uTorrent for Mac beta officially released'), Row(Title='$1.7 million for Greensboro Apple Store permit'), Row(Title='Details emerge on future Apple Stores')]\n"
     ]
    }
   ],
   "source": [
    "# flatmap Transformation\n",
    "\n",
    "df_hdfs = sc.read.option('delimiter', ',').option('header', 'true')\\\n",
    "    .csv(\"/user/bdccuser/posts*\")\n",
    "\n",
    "rdd = df_hdfs.rdd\n",
    "\n",
    "print(rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acute-dress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat RDD Posts: ['title', 'first', 'look', 'guitar', 'rock', 'tour', 'for', 'iphone', 'tuaw', 'tip', 'use', 'the', 'help', 'menu', 'to', 'search', 'safari', 'bookmarks', 'and', 'history', 'boxee', 'is', 'updated', 'for', 'apple', 'tv', '23', 'found', 'footage', 'a', 'working', 'next', 'cube', 'concept', 'the', 'ipod', 'shuffle', 'bracelet', 'talkcast', 'live', 'tonight', 'at', '10pm', 'et', 'black', 'friday', 'best', 'buy', 'discounting', 'up']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(x) :\n",
    "    posts = x.split()\n",
    "    for k in range(len(posts)) :\n",
    "        # Convert to lower case\n",
    "        posts[k] = posts[k].lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        posts[k] = re.sub(r'[^\\w\\s]', '', posts[k])\n",
    "       \n",
    "\n",
    "    return posts\n",
    "\n",
    "rdd_posts_flat_2 = rdd_posts_hdfs.flatMap( tokenize )\n",
    "\n",
    "print(\"Flat RDD Posts:\", rdd_posts_flat_2.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "educational-richardson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with 1s: [('title', 1), ('first', 1), ('look', 1), ('guitar', 1), ('rock', 1), ('tour', 1), ('for', 1), ('iphone', 1), ('tuaw', 1), ('tip', 1), ('use', 1), ('the', 1), ('help', 1), ('menu', 1), ('to', 1), ('search', 1), ('safari', 1), ('bookmarks', 1), ('and', 1), ('history', 1), ('boxee', 1), ('is', 1), ('updated', 1), ('for', 1), ('apple', 1), ('tv', 1), ('23', 1), ('found', 1), ('footage', 1), ('a', 1), ('working', 1), ('next', 1), ('cube', 1), ('concept', 1), ('the', 1), ('ipod', 1), ('shuffle', 1), ('bracelet', 1), ('talkcast', 1), ('live', 1), ('tonight', 1), ('at', 1), ('10pm', 1), ('et', 1), ('black', 1), ('friday', 1), ('best', 1), ('buy', 1), ('discounting', 1), ('up', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Combining flatmap and map\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize(x) :\n",
    "    posts = x.split()\n",
    "    for k in range(len(posts)) :\n",
    "        # Convert to lower case\n",
    "        posts[k] = posts[k].lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        posts[k] = re.sub(r'[^\\w\\s]', '', posts[k])\n",
    "\n",
    "    return posts\n",
    "\n",
    "rdd_posts_flat = rdd_posts_hdfs.flatMap( tokenize )\n",
    "rdd_posts = rdd_posts_flat.map(lambda x: (x,1))\n",
    "\n",
    "print(\"Posts with 1s:\", rdd_posts.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blank-homeless",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts counts: [('title', 4), ('look', 161), ('tour', 27), ('use', 116), ('help', 48), ('search', 45), ('is', 532), ('footage', 170), ('working', 37), ('cube', 27), ('live', 89), ('at', 385), ('10pm', 5), ('friday', 49), ('best', 194), ('150', 1), ('match', 6), ('utorrent', 2), ('mac', 1766), ('17', 19), ('store', 636), ('details', 48), ('stores', 91), ('of', 1243), ('ask', 128), ('reinstalling', 2), ('more', 409), ('sneak', 26), ('preview', 54), ('', 487), ('freeverse', 26), ('12', 61), ('goes', 139), ('last', 38), ('chance', 5), ('two', 82), ('freebies', 7), ('hackers', 9), ('boot', 68), ('doorbusters', 1), ('holiday', 28), ('savings', 4), ('love', 49), ('yellow', 11), ('australia', 13), ('sale', 42), ('757', 1), ('issues', 28), ('says', 64), ('paul', 11)]\n"
     ]
    }
   ],
   "source": [
    "##The reduceByKey transformation\n",
    "#(word, frequency)\n",
    "\n",
    "rdd_reduced = rdd_posts.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"Posts counts:\", rdd_reduced.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "velvet-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversed RDD: [(4, 'title'), (161, 'look'), (27, 'tour'), (116, 'use'), (48, 'help'), (45, 'search'), (532, 'is'), (170, 'footage'), (37, 'working'), (27, 'cube'), (89, 'live'), (385, 'at'), (5, '10pm'), (49, 'friday'), (194, 'best'), (1, '150'), (6, 'match'), (2, 'utorrent'), (1766, 'mac'), (19, '17'), (636, 'store'), (48, 'details'), (91, 'stores'), (1243, 'of'), (128, 'ask'), (2, 'reinstalling'), (409, 'more'), (26, 'sneak'), (54, 'preview'), (487, ''), (26, 'freeverse'), (61, '12'), (139, 'goes'), (38, 'last'), (5, 'chance'), (82, 'two'), (7, 'freebies'), (9, 'hackers'), (68, 'boot'), (1, 'doorbusters'), (28, 'holiday'), (4, 'savings'), (49, 'love'), (11, 'yellow'), (13, 'australia'), (42, 'sale'), (1, '757'), (28, 'issues'), (64, 'says'), (11, 'paul')]\n"
     ]
    }
   ],
   "source": [
    "# Transform our reduced RDD from its (Word, Frequency) format, to (Frequency, Word)\n",
    "rdd_reduced_rev = rdd_reduced.map(lambda x: ( x[1], x[0] )) \n",
    "print(\"Reversed RDD:\", rdd_reduced_rev.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "olive-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency sorted Word counts: [(2785, 'the'), (2278, 'apple'), (2015, 'to'), (1916, 'for'), (1766, 'mac'), (1615, 'iphone'), (1556, 'ipod'), (1315, 'on'), (1258, 'a'), (1243, 'of'), (1199, 'and'), (1179, 'your'), (1094, 'in'), (1035, 'itunes'), (1025, 'with'), (775, 'tuaw'), (731, 'new'), (636, 'store'), (532, 'is'), (487, ''), (481, 'os'), (461, 'x'), (426, 'from'), (409, 'more'), (397, 'update'), (385, 'at'), (385, 'an'), (375, 'free'), (371, 'you'), (367, 'video'), (344, 'now'), (344, 'macbook'), (327, 'widget'), (309, 'leopard'), (306, 'available'), (303, 'beta'), (291, 'pro'), (260, 'mini'), (259, 'watch'), (252, 'released'), (250, 'up'), (249, 'releases'), (248, 'get'), (239, 'first'), (220, 'macworld'), (217, '101'), (215, '2'), (214, 'updated'), (212, '10'), (211, 'tv')]\n"
     ]
    }
   ],
   "source": [
    "# Sort our reduced RDD in decreasing frequency order\n",
    "rdd_sorted_freq = rdd_reduced_rev.sortByKey(False)\n",
    "print(\"Frequency sorted Word counts:\", rdd_sorted_freq.take(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
